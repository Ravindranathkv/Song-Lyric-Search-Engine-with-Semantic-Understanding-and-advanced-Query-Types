{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note to graders\n",
    "#### For lemmatizing process, words like won't, can't, don't are tokenized as wo, ca, do, n't. I could have handled it by removing those but I felt they carry important information. So, I expanded all the contractions and tokenized. Results might slightly differ because of that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\ravin\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\ravin\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\ravin\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\ravin\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "!pip install contractions\n",
    "import contractions\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAxczUGVN-Jc"
   },
   "source": [
    "#### CSCE 670 :: Information Storage & Retrieval :: Texas A&M University :: Spring 2023\n",
    "\n",
    "\n",
    "# Homework 1:  Lyrical Search Engine\n",
    "\n",
    "### 100 points [9% of your final grade]\n",
    "\n",
    "### Due: February 10 (Friday) by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine. In particular, there are three main learning objectives: (i) the basics of tokenization (e.g. stemming, case-folding, etc.) and its effect on information retrieval; (ii) basics of index building and Boolean retrieval; and (iii) basics of the Vector Space model and ranked retrieval.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw1.ipynb`. For example, my homework submission would be something like `555001234_hw1.ipynb`. Submit this notebook via eCampus (look for the homework 1 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CKllFl-N-Jk"
   },
   "source": [
    "# Dataset: Genius Lyrics Dataset\n",
    "\n",
    "We are providing you with a small collection of the lyrics to 200 songs collected from Genius (https://genius.com/). The full data was originally collected by Austin Benson at Cornell (https://www.cs.cornell.edu/~arb/data/genius-expertise/). For this homework, you can use just the small set we provide: **lyrics_200.jl**. You should treat each song as a unique document to be indexed by your system. You can download the data from eCampus to your local filesystem. We're going to use these lyrics as the basis of a Lyrical Search Engine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaiOjFANN-Jo"
   },
   "source": [
    "# Part 1: Read and Parse the Lyrics Data (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoiNbqQDrVQ2"
   },
   "source": [
    "Recall how we handled file input in Homework 0? Well, here, our goal is to read the lyrics so that we can begin to tokenize them later. For this step, you should read the dataset and print the lyrics. Note that our dataset is in JSON lines format, meaning that each line break separates an entry in JSON format. A document looks like:\n",
    "\n",
    "{'songs': 'Linkin-park-in-the-end-lyrics', 'lyrics': '\\n\\n[Verse 1: Mike Shinoda & Chester Bennington]\\nIt starts...'}\n",
    "\n",
    "For this homework, you should treat the lyrics as a document and the songs as the document ID.\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "DocumentID Document\n",
    "\n",
    "Linkin-park-in-the-end-lyrics \\n\\n[Verse 1: Mike Shinoda & Chester Bennington]\\nIt starts...\n",
    "\n",
    "... ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IfHGvybCysQK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erykah-badu-kiss-me-on-my-neck-lyrics</td>\n",
       "      <td>\\n\\n[Production by J Dilla, Erykah Badu, and J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daniel-caesar-we-find-love-lyrics</td>\n",
       "      <td>\\n\\n[Verse 1]\\nYou don't love me anymore\\nLet'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florence-the-machine-hunger-lyrics</td>\n",
       "      <td>\\n\\n[Intro]\\nOoh, ooh, ooh-ooh, ooh\\nOoh, ooh,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael-jackson-will-you-be-there-lyrics</td>\n",
       "      <td>\\n\\n[Verse 1]\\nHold me\\nLike the River Jordan\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Partynextdoor-not-nice-lyrics</td>\n",
       "      <td>\\n\\n[Intro: PARTYNEXTDOOR]\\nOh-oh-oh-oh\\nOh-oh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 DocumentID  \\\n",
       "0     Erykah-badu-kiss-me-on-my-neck-lyrics   \n",
       "1         Daniel-caesar-we-find-love-lyrics   \n",
       "2        Florence-the-machine-hunger-lyrics   \n",
       "3  Michael-jackson-will-you-be-there-lyrics   \n",
       "4             Partynextdoor-not-nice-lyrics   \n",
       "\n",
       "                                            Document  \n",
       "0  \\n\\n[Production by J Dilla, Erykah Badu, and J...  \n",
       "1  \\n\\n[Verse 1]\\nYou don't love me anymore\\nLet'...  \n",
       "2  \\n\\n[Intro]\\nOoh, ooh, ooh-ooh, ooh\\nOoh, ooh,...  \n",
       "3  \\n\\n[Verse 1]\\nHold me\\nLike the River Jordan\\...  \n",
       "4  \\n\\n[Intro: PARTYNEXTDOOR]\\nOh-oh-oh-oh\\nOh-oh...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def json_to_df(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "#print(cur_dir)\n",
    "file_path = str(cur_dir) + '/lyrics_200.jl'\n",
    "df = json_to_df(file_path)\n",
    "col_names = ['DocumentID', 'Document']\n",
    "df.columns = col_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax-juuk1zLuw"
   },
   "source": [
    "Now that you can read the documents, let's move on to tokenization. You should lowercase all words. Your parser needs to also provide the following three pre-processing options:\n",
    "1. Remove background vocals (strings in parentheses) and song structure indicators \n",
    "(strings in square brackets, e.g., [Verse 1: Mike Shinoda & Chester Bennington])\n",
    "2. Lemmatization: use nltk Lemmatizer `from nltk.stem import WordNetLemmatizer`\n",
    "3. Remove any other strings that you think are less informative or noisy, e.g., non-word vocal sounds, non-character.\n",
    "\n",
    "Please note that you should stick to the stemming package listed above. Otherwise, given the same query, the results generated by your code can be different from others. For tokenization, you could use [nltk.tokenize.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize) or you can write your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tst9U9DxN-Jp"
   },
   "source": [
    "## Observations (3 * 4 = 12 points)\n",
    "\n",
    "Once you have your parser working, you should \n",
    "\n",
    "* print the first two documents (documentID and tokens);\n",
    "\n",
    "* report the size of your dictionary, that is, how many unique tokens;\n",
    "\n",
    "* print a list of the top-10 most popular words by count;\n",
    "\n",
    "under the four cases:\n",
    "\n",
    "* None of pre-processing options (that is, just whatever nltk.tokenize.word_tokenize gives you)\n",
    "* (1) Remove background vocals\n",
    "* (1) + (2) Remove background vocals + Lemmatization\n",
    "* (1) + (2) + (3) Remove background vocals  + Lemmatization + Remove other\n",
    "\n",
    "Your output should look like this:\n",
    "\n",
    "* DocumentID Tokens\n",
    "\n",
    "* Unique token numbers:\n",
    "\n",
    "* Rank Token Count\n",
    "\n",
    "   1    awesome    20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B6Kk0ExbJBit"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 0 for no pre-processing options\n",
      "Press 1 for Removing background vocals\n",
      "Press 2 for Removing background vocals and Lemmatization\n",
      "Press 3 for Removing background vocals, Lemmatization and Other unwanted strings\n",
      "Enter your option: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Erykah-badu-kiss-me-on-my-neck-lyrics</td>\n",
       "      <td>[production by j dilla, erykah badu, and jam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daniel-caesar-we-find-love-lyrics</td>\n",
       "      <td>[verse 1] you don't love me anymore let's se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florence-the-machine-hunger-lyrics</td>\n",
       "      <td>[intro] ooh, ooh, ooh-ooh, ooh ooh, ooh, ooh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael-jackson-will-you-be-there-lyrics</td>\n",
       "      <td>[verse 1] hold me like the river jordan and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Partynextdoor-not-nice-lyrics</td>\n",
       "      <td>[intro: partynextdoor] oh-oh-oh-oh oh-oh-oh-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 DocumentID  \\\n",
       "0     Erykah-badu-kiss-me-on-my-neck-lyrics   \n",
       "1         Daniel-caesar-we-find-love-lyrics   \n",
       "2        Florence-the-machine-hunger-lyrics   \n",
       "3  Michael-jackson-will-you-be-there-lyrics   \n",
       "4             Partynextdoor-not-nice-lyrics   \n",
       "\n",
       "                                            Document  \n",
       "0    [production by j dilla, erykah badu, and jam...  \n",
       "1    [verse 1] you don't love me anymore let's se...  \n",
       "2    [intro] ooh, ooh, ooh-ooh, ooh ooh, ooh, ooh...  \n",
       "3    [verse 1] hold me like the river jordan and ...  \n",
       "4    [intro: partynextdoor] oh-oh-oh-oh oh-oh-oh-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your parser function here. It will take the three option variables above as the parameters.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "df['Document'] = df['Document'].str.lower()\n",
    "df['Document'] = df['Document'].str.replace(\"\\n\", \" \")\n",
    "print(\"Press 0 for no pre-processing options\")\n",
    "print(\"Press 1 for Removing background vocals\")\n",
    "print(\"Press 2 for Removing background vocals and Lemmatization\")\n",
    "print(\"Press 3 for Removing background vocals, Lemmatization and Other unwanted strings\")\n",
    "# add cells as needed to organize your code\n",
    "option = int(input(\"Enter your option: \"))\n",
    "while option not in [0, 1, 2, 3]:\n",
    "    option = int(input(\"Wrong option, Enter your option again: \"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Useful Imports and Functions\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "punctuation = ['!', '#', \"'\", '\"', '$', '%', '&', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '\\\\', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "def remove_bgvocals(text):\n",
    "    return re.sub(r'[\\[\\(].*?[\\)\\]]', '', text)\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    no_punct = []\n",
    "    #words = words.split()\n",
    "    for word in words:\n",
    "        no_punct_word = \"\"\n",
    "        for char in word:\n",
    "            if char not in punctuation:\n",
    "                no_punct_word += char\n",
    "        if no_punct_word.isalpha():\n",
    "            no_punct.append(no_punct_word)\n",
    "    return no_punct\n",
    "\n",
    "def remove_stop_words(wordlist):\n",
    "    return [word for word in wordlist if word.lower() not in stop_words]\n",
    "\n",
    "def lemmatize_words(wordlist):\n",
    "    return [wordnet_lemmatizer.lemmatize(word) for word in wordlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hv6QtgLIHUgL"
   },
   "source": [
    "#### None of Pre-processing Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WcJbBEQgJUd8"
   },
   "outputs": [],
   "source": [
    "# your code and output here\n",
    "tokens = []\n",
    "if option == 0:\n",
    "    for string in df['Document']:\n",
    "        words = word_tokenize(string)\n",
    "        tokens.append(words)\n",
    "    df['Tokens'] = tokens\n",
    "    final_dict = [word for words in tokens for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxjPoaWUIFkx"
   },
   "source": [
    "### (1) Remove background vocals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uDbliiXUILHo"
   },
   "outputs": [],
   "source": [
    "# your code and output here\n",
    "tokens = []\n",
    "if option == 1:\n",
    "    df['Document'] = df['Document'].apply(remove_bgvocals) #Remove BgVocals\n",
    "    for string in df['Document']:\n",
    "        expanded_words = []   \n",
    "        for word in string.split():\n",
    "            expanded_words.append(contractions.fix(word)) #Expand Words (taking care of contractions)\n",
    "        expanded_text = ' '.join(expanded_words)\n",
    "        words = word_tokenize(expanded_text)\n",
    "        tokens.append(words)\n",
    "    df['Tokens'] = tokens\n",
    "    final_dict = [word for words in tokens for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JnT2kepITqG"
   },
   "source": [
    "### (1) + (2) Remove background vocals + Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IulY8NZuIep8"
   },
   "outputs": [],
   "source": [
    "# your code and output here\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = []\n",
    "if option == 2:\n",
    "    df['Document'] = df['Document'].apply(remove_bgvocals) #Remove BgVocals\n",
    "    for string in df['Document']:\n",
    "        expanded_words = []   \n",
    "        for word in string.split():\n",
    "            expanded_words.append(contractions.fix(word)) #Expand Words (taking care of contractions)\n",
    "        expanded_text = ' '.join(expanded_words)\n",
    "        words = word_tokenize(expanded_text)\n",
    "        tokens.append(words)\n",
    "    df['Tokens'] = tokens  \n",
    "    df['Tokens'] = df['Tokens'].apply(lemmatize_words) #lemmatize\n",
    "    dictionary = df.Tokens.tolist()\n",
    "    final_dict = [word for words in dictionary for word in words]\n",
    "    len(final_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOKY3uUuIjd4"
   },
   "source": [
    "### (1) + (2) + (3) Remove background vocals  + Lemmatization + Remove other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D76gCFlVItyd"
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "if option == 3:\n",
    "    df['Document'] = df['Document'].apply(remove_bgvocals) #Remove BgVocals\n",
    "    for string in df['Document']:\n",
    "        expanded_words = []   \n",
    "        for word in string.split():\n",
    "            expanded_words.append(contractions.fix(word)) #Expand Words (taking care of contractions)\n",
    "        expanded_text = ' '.join(expanded_words)\n",
    "        words = word_tokenize(expanded_text)\n",
    "        tokens.append(words)     \n",
    "    df['Tokens'] = tokens  \n",
    "    df['Tokens'] = df['Tokens'].apply(lemmatize_words) #lemmatize\n",
    "    df['Tokens'] = df['Tokens'].apply(remove_punctuation) #Remove Punctuation\n",
    "    df['Tokens'] = df['Tokens'].apply(remove_stop_words) #Remove Stop Words\n",
    "    dictionary = df.Tokens.tolist()\n",
    "    final_dict = [word for words in dictionary for word in words]\n",
    "    len(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT\n",
      "===========================================================================================================\n",
      "Document id : Erykah-badu-kiss-me-on-my-neck-lyrics\n",
      "Tokens: ['i', 'want', 'somebody', 'to', 'walk', 'up', 'behind', 'me', 'and', 'kiss', 'me', 'on', 'my', 'neck', 'and', 'breathe', 'on', 'my', 'neck', 'i', 'want', 'somebody', 'to', 'walk', 'up', 'behind', 'me', 'and', 'kiss', 'me', 'on', 'my', 'neck', 'and', 'breathe', 'on', 'my', 'neck', 'been', 'such', 'a', 'long', 'time', 'i', 'forgot', 'that', 'i', 'wa', 'fine', 'just', 'kiss', 'me', 'on', 'my', 'neck', 'and', 'breathe', 'on', 'my', 'neck', 'i', 'want', 'somebody', 'to', 'walk', 'up', 'behind', 'me', 'and', 'kiss', 'me', 'on', 'my', 'neck', 'and', 'breathe', 'on', 'my', 'neck', 'if', 'you', 'want', 'to', 'feel', 'me', 'better', 'be', 'divine', 'bring', 'me', 'water', ',', 'water', 'for', 'my', 'mind', 'give', 'me', 'nothing', 'breathe', 'love', 'in', 'my', 'air', 'do', 'not', 'abuse', 'me', 'because', 'these', 'herb', 'are', 'rare', 'if', 'you', 'want', 'to', 'feel', 'me', 'better', 'be', 'divine', 'bring', 'me', 'water', ',', 'water', 'for', 'my', 'mind', 'give', 'me', 'nothing', 'breathe', 'love', 'in', 'my', 'air', 'do', 'not', 'abuse', 'me', 'because', 'these', 'herb', 'are', 'rare', 'if', 'you', 'want', 'to', 'feel', 'me', 'baby', 'better', 'be', 'divine', 'bring', 'me', 'water', 'for', 'these', 'flower', 'growing', 'out', 'my', 'mind', 'give', 'me', 'nothing', 'just', 'be', 'gentle', 'breathe', 'love', 'in', 'my', 'air', 'use', 'me', ',', 'do', 'not', 'abuse', 'me', ',', 'love', 'me', 'because', 'these', 'herb', 'are', 'rare', 'if', 'you', 'want', 'to', 'feel', 'me', 'better', 'be', 'divine', 'bring', 'me', 'water', ',', 'water', 'for', 'my', 'mind', 'give', 'me', 'nothing', 'breathe', 'love', 'in', 'my', 'air', 'do', 'not', 'abuse', 'me', 'because', 'these', 'herb', 'are', 'rare', 'if', 'you', 'want', 'to', 'feel', 'me', 'better', 'be', 'divine', 'bring', 'me', 'water', ',', 'water', 'for', 'my', 'mind', 'give', 'me', 'nothing', 'breathe', 'love', 'in', 'my', 'air', 'do', 'not', 'abuse', 'me', 'because', 'these', 'herb', 'are', 'rare', 'if', 'you', 'want', 'to', 'feel', 'me', 'better', 'be', 'divine', 'bring', 'me', 'water', ',', 'water', 'for', 'my', 'mind', 'give', 'me', 'nothing', 'breathe', 'love', 'in', 'my', 'air', 'do', 'not', 'abuse', 'me', 'because', 'these', 'herb', 'are', 'rare', 'i', 'want', 'somebody', 'to', 'walk', 'up', 'behind', 'me', 'and', 'kiss', 'me', 'on', 'my', 'neck', 'and', 'breathe', 'on', 'my', 'neck', 'i', 'want', 'somebody', 'to', 'walk', 'up', 'behind', 'me', 'and', 'kiss', 'me', 'on', 'my', 'neck', 'and', 'breathe', 'on', 'my', 'neck', 'been', 'such', 'a', 'long', 'time', 'i', 'forgot', 'that', 'i', 'wa', 'fine', 'just', 'kiss', 'me', 'on', 'my', 'neck', 'and', 'breathe', 'on', 'my', 'neck', 'i', 'want', 'somebody', 'to', 'walk', 'up', 'behind', 'me', 'and', 'kiss', 'me', 'on', 'my', 'neck', 'and', 'breathe', 'on', 'my', 'neck']\n",
      "===========================================================================================================\n",
      "Document id : Daniel-caesar-we-find-love-lyrics\n",
      "Tokens: ['you', 'do', 'not', 'love', 'me', 'anymore', 'let', 'u', 'see', 'how', 'you', 'like', 'this', 'song', 'see', 'you', 'walking', 'out', 'that', 'door', 'wonder', 'why', 'it', 'took', 'you', 'so', 'long', 'ever', 'since', 'the', 'day', 'that', 'i', 'met', 'you', 'i', 'knew', 'you', 'were', 'the', 'girl', 'of', 'my', 'dream', 'but', 'we', 'could', 'never', 'be', 'you', 'do', 'not', 'love', 'me', 'anymore', 'let', 'u', 'see', 'how', 'you', 'like', 'this', 'song', 'we', 'find', 'love', ',', 'we', 'get', 'up', 'then', 'we', 'fall', 'down', ',', 'we', 'give', 'up', 'we', 'find', 'love', ',', 'we', 'get', 'up', 'then', 'we', 'fall', 'down', ',', 'we', 'give', 'up', 'you', 'do', 'not', 'love', 'me', 'anymore', 'let', 'u', 'see', 'how', 'you', 'like', 'this', 'song', 'you', 'need', 'someone', 'you', 'adore', 'find', 'a', 'place', 'where', 'you', 'might', 'belong', 'ever', 'since', 'the', 'day', 'that', 'i', 'met', 'you', 'my', 'world', \"'s\", 'been', 'spinnin', \"'\", 'out', 'of', 'control', 'i', 'just', 'need', 'you', 'to', 'hold', 'we', 'knew', 'it', 'would', 'come', 'around', 'this', 'thing', 'called', 'love', 'come', 'crashing', 'down', ',', 'ahh', 'piece', 'all', 'on', 'the', 'ground', 'what', 'once', 'wa', 'lost', 'can', 'not', 'be', 'found', ',', 'ahh', 'we', 'knew', 'it', 'would', 'come', 'around', 'this', 'thing', 'called', 'love', 'come', 'crashing', 'down', ',', 'ahh', 'piece', 'all', 'on', 'the', 'ground', 'what', 'once', 'wa', 'lost', 'can', 'not', 'be', 'found', ',', 'ahh', 'we', 'knew', 'it', 'would', 'come', 'around', 'this', 'thing', 'called', 'love', 'come', 'crashing', 'down', ',', 'ahh', 'piece', 'all', 'on', 'the', 'ground', 'what', 'once', 'wa', 'lost', 'can', 'not', 'be', 'found', ',', 'ahh', 'heaven', 'help', 'u', ',', 'heaven', 'help', 'u', ',', 'yeah', 'we', 'are', 'on', 'our', 'own', 'we', 'find', 'love', ',', 'we', 'get', 'up', 'then', 'we', 'fall', 'down', ',', 'we', 'give', 'up', 'we', 'find', 'love', ',', 'we', 'get', 'up', 'then', 'we', 'fall', 'down', ',', 'we', 'give', 'up', 'we', 'find', 'love', ',', 'we', 'get', 'up', 'then', 'we', 'fall', 'down', ',', 'we', 'give', 'up', 'we', 'find', 'love', ',', 'yeah', ',', 'we', 'get', 'up', 'we', 'fall', 'down', ',', 'we', 'give', 'up']\n",
      "===========================================================================================================\n",
      "Size of Dictionary is : 4423\n",
      "Top 10 words along with their frequency:  [(',', 4611), ('i', 3389), ('you', 2842), ('the', 2350), ('to', 1594), ('and', 1304), ('me', 1195), ('not', 1179), ('a', 1162), ('it', 1151)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"OUTPUT\")\n",
    "print(\"===========================================================================================================\")\n",
    "print(\"Document id : {}\\nTokens: {}\".format(df.DocumentID[0], df.Tokens[0]))\n",
    "print(\"===========================================================================================================\")\n",
    "print(\"Document id : {}\\nTokens: {}\".format(df.DocumentID[1], df.Tokens[1]))\n",
    "word_frequency = Counter(final_dict)\n",
    "sorted_word_frequency = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"===========================================================================================================\")\n",
    "print(\"Size of Dictionary is :\", len(sorted_word_frequency))\n",
    "print(\"Top 10 words along with their frequency: \", sorted_word_frequency[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goyeyYtcDAnj"
   },
   "source": [
    "## Zipf's Law (8 points)\n",
    "Recall in class our discussion of Zipf's law. Let's see if this law applies to our Genius Lyrics. You should use matplotlib to plot the log-base10 term counts on the y-axis versus the log-base10 rank on the x-axis. Your aim is to create a figure like the one in Figure 5.2 of the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TOOwjhLzDf5q"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def zipf_law(words):\n",
    "    word_frequency = Counter(words)\n",
    "    sorted_word_frequency = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "    rank = np.array([i+1 for i in range(len(sorted_word_frequency))])\n",
    "    frequency = np.array([x[1] for x in sorted_word_frequency])\n",
    "\n",
    "    plt.loglog(rank, frequency, marker=\"o\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"log(Rank)\")\n",
    "    plt.ylabel(\"log(Frequency)\")\n",
    "    plt.title(\"Zipf's Law for words in text\")\n",
    "    plt.show()\n",
    "\n",
    "zipf_law(final_dict)\n",
    "doc_dict = set(final_dict)\n",
    "doc_dict = list(doc_dict)\n",
    "doc_dict.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks3jMhkLzzoK"
   },
   "source": [
    "What do you observe? Explain.  \n",
    "\n",
    "Zipf's law states that the frequency of a word in a text corpus is proportional to its rank in the frequency table of the corpus, with the most frequent word having a rank of 1. This means that the most frequent word in a corpus is likely to be much more common than the second most frequent word, which is in turn much more common than the third most frequent word, and so on.  \n",
    "From the above plot, we can observe a slope that decreases linearly from left to right. This slope represents the power-law relationship between frequency and rank. The steeper the slope, the stronger the relationship. As the slope is close to -1, this means that the frequency of elements decreases very rapidly as their rank increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcrFD2F-N-Jp"
   },
   "source": [
    "# Part 2: Boolean Retrieval (30 points)\n",
    "\n",
    "In this part you will build an inverted index to support Boolean retrieval. You should use the (1) + (2) tokenization strategy from above (Remove background vocals + Lemmatization). \n",
    "\n",
    "We only require your index to support AND queries. In other words, your index does not have to support OR, NOT, or parentheses. Also, we do not explicitly expect to see AND in queries, e.g., when we query **relational model**, your search engine should treat it as **relational** AND **model**.\n",
    "\n",
    "Search for the queries below using your index and print out matching documents (for each query, print out 5 matching documents):\n",
    "* time\n",
    "* never know\n",
    "* make no sense\n",
    "\n",
    "Recall, that you should apply the exact same pre-processing strategies to the query as we do to the documents. \n",
    "\n",
    "The output should like this:\n",
    "* DocumentID Document\n",
    "\n",
    "To make our life easier, please output the DocumentIDs in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class InvertedIndex: \n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(list)\n",
    "\n",
    "    def add_document(self, doc_id, terms):\n",
    "        for term in set(terms):\n",
    "            self.index[term].append(doc_id)\n",
    "    \n",
    "    def search(self, query):\n",
    "        terms = query.split()\n",
    "        results = None\n",
    "\n",
    "        for term in terms:\n",
    "            if term not in self.index:\n",
    "                return []\n",
    "            if results is None:\n",
    "                results = set(self.index[term])\n",
    "            else:\n",
    "                results &= set(self.index[term])\n",
    "                \n",
    "        results = list(results)\n",
    "        results.sort()\n",
    "        return results\n",
    "\n",
    "index = InvertedIndex()\n",
    "\n",
    "# Add all documents to the index\n",
    "for i in range(len(df)):\n",
    "    index.add_document(df['DocumentID'][i], df['Tokens'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GS3Tc_kYN-Jp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# list of words to query\n",
    "query = [\"time\", \"never know\", \"make no sense\"]\n",
    "queries = []\n",
    "\n",
    "# Perform remove bg vocals and lemmatization\n",
    "for q in query:\n",
    "    q = remove_bgvocals(q)\n",
    "    q = list(q.split())\n",
    "    q = lemmatize_words(q)\n",
    "    queries.append(q)\n",
    "queries\n",
    "\n",
    "# Function to build query from tokens to search\n",
    "def combine_words_for_and_query(words):\n",
    "    return \" \".join(words)\n",
    "\n",
    "search_queries = []\n",
    "for query in queries:\n",
    "    search_queries.append(combine_words_for_and_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsA7pR87LnE3"
   },
   "source": [
    "Now show the results for the query: `time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qWH8h5ZkN-Jp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 docs in alphabetical order for Query: time\n",
      "['5-seconds-of-summer-close-as-strangers-lyrics', 'Alessia-cara-outlaws-lyrics', 'Alicia-keys-fallin-lyrics', 'Alicia-keys-no-one-lyrics', 'Anitta-goals-lyrics']\n"
     ]
    }
   ],
   "source": [
    "#Top 5 results for Query1\n",
    "print(\"5 docs in alphabetical order for Query: {}\\n{}\".format(search_queries[0], index.search(search_queries[0])[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbJ6yn3yMQWY"
   },
   "source": [
    "Now show the results for the query: `never know`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SmF62rQ_MRlO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 docs in alphabetical order for Query: never know\n",
      "['2pac-u-r-ripping-us-apart-dedicated-2-crack-annotated', '5-seconds-of-summer-heartbreak-girl-lyrics', 'Alessia-cara-outlaws-lyrics', 'Alessia-cara-the-other-side-lyrics', 'Allie-x-sunflower-synth-reprise-lyrics']\n"
     ]
    }
   ],
   "source": [
    "#Top 5 results for Query2\n",
    "print(\"5 docs in alphabetical order for Query: {}\\n{}\".format(search_queries[1], index.search(search_queries[1])[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8KlaH0fMTO5"
   },
   "source": [
    "Now show the results for the query: `make no sense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MAFBtPGmMVRW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 docs in alphabetical order for Query: make no sense\n",
      "['Bring-me-the-horizon-mother-tongue-lyrics', 'Devlin-all-along-the-watchtower-lyrics', 'Florence-the-machine-all-this-and-heaven-too-lyrics', 'Kid-cudi-ghost-lyrics', 'Linkin-park-one-step-closer-lyrics']\n"
     ]
    }
   ],
   "source": [
    "#Top 5 results for Query3\n",
    "print(\"5 docs in alphabetical order for Query: {}\\n{}\".format(search_queries[2], index.search(search_queries[2])[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAhHuIDfN-Jp"
   },
   "source": [
    "## Observations (6 points)\n",
    "Could your boolean search engine find relevant documents for these queries? What is the impact of the pre-processing options? Do they improve your search quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX6k-Vil0GIy"
   },
   "source": [
    "Yes, a boolean search engine can find relevant documents for queries by using the inverted index to efficiently locate documents that match the query terms.  \n",
    "\n",
    "The impact of pre-processing options such as lemmatization can greatly improve the quality of the search results in a boolean retrieval system. By reducing words to their base form, lemmatization can make it easier for the search engine to match query terms with terms in the documents, as well as reduce the size of the index. This can lead to more relevant results, as well as faster and more efficient searches.  \n",
    "\n",
    "Pre-processing options improved the search quality by reducing the noise in the index and making the search more focused on the meaningful terms in the query and documents.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8c7A4nuN-Jp"
   },
   "source": [
    "# Part 3: Ranking Documents (40 points) \n",
    "\n",
    "In this part, your job is to rank the documents rather than just provide set-based results as in Boolean Retrieval.\n",
    "\n",
    "### A: Ranking with simple sums of TF-IDF scores (10 points) \n",
    "For a multi-word query, we rank documents by a simple sum of the TF-IDF scores for the query terms in the document.\n",
    "TF is the log-weighted term frequency $1+log(tf)$; and IDF is the log-weighted inverse document frequency $log(\\frac{N}{df})$\n",
    "\n",
    "**Output:**\n",
    "You should output the top-5 results plus the TF-IDF sum score of each of these documents. \n",
    "\n",
    "The output should be like this:\n",
    "\n",
    "Rank Scores DocumentID Document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Search number of times a term appears in document\n",
    "def term_frequency(document, term):\n",
    "    return 1 + math.log(document.count(term))/math.log(10)\n",
    "\n",
    "# Counts number of rows a term appear in Tokens\n",
    "def count_rows(df, term):\n",
    "    return sum([1 for row in df['Tokens'] if term in row])\n",
    "\n",
    "# Return inverted index of a term\n",
    "def idf(term, df):\n",
    "    if count_rows(df, term) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log(len(df)/count_rows(df, term))/math.log(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-ESKUvzkQ1R"
   },
   "source": [
    "Now show the results for the query: `time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SK68cvtDN-Js"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Query_Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek-minor-until-the-end-of-time-lyrics</td>\n",
       "      <td>time is the fourth dimension and a measure ...</td>\n",
       "      <td>[time, is, the, fourth, dimension, and, a, mea...</td>\n",
       "      <td>0.997799</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John-legend-for-the-first-time-lyrics</td>\n",
       "      <td>is that music in my ear it's like i heard...</td>\n",
       "      <td>[is, that, music, in, my, ear, it, is, like, i...</td>\n",
       "      <td>0.938886</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Method-man-if-time-is-money-fly-navigation-lyrics</td>\n",
       "      <td>uh, yeah choose a side   look, vibe with me...</td>\n",
       "      <td>[uh, ,, yeah, choose, a, side, look, ,, vibe, ...</td>\n",
       "      <td>0.900474</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linkin-park-from-the-inside-lyrics</td>\n",
       "      <td>i don't know who to trust, no surprise ever...</td>\n",
       "      <td>[i, do, not, know, who, to, trust, ,, no, surp...</td>\n",
       "      <td>0.810657</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Partynextdoor-not-nice-lyrics</td>\n",
       "      <td>oh-oh-oh-oh oh-oh-oh-oh   you've got some v...</td>\n",
       "      <td>[oh-oh-oh-oh, oh-oh-oh-oh, you, have, got, som...</td>\n",
       "      <td>0.810657</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          DocumentID  \\\n",
       "0           Derek-minor-until-the-end-of-time-lyrics   \n",
       "1              John-legend-for-the-first-time-lyrics   \n",
       "2  Method-man-if-time-is-money-fly-navigation-lyrics   \n",
       "3                 Linkin-park-from-the-inside-lyrics   \n",
       "4                      Partynextdoor-not-nice-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     time is the fourth dimension and a measure ...   \n",
       "1       is that music in my ear it's like i heard...   \n",
       "2     uh, yeah choose a side   look, vibe with me...   \n",
       "3     i don't know who to trust, no surprise ever...   \n",
       "4     oh-oh-oh-oh oh-oh-oh-oh   you've got some v...   \n",
       "\n",
       "                                              Tokens  Query_Score  Rank  \n",
       "0  [time, is, the, fourth, dimension, and, a, mea...     0.997799   1.0  \n",
       "1  [is, that, music, in, my, ear, it, is, like, i...     0.938886   2.0  \n",
       "2  [uh, ,, yeah, choose, a, side, look, ,, vibe, ...     0.900474   3.0  \n",
       "3  [i, do, not, know, who, to, trust, ,, no, surp...     0.810657   5.0  \n",
       "4  [oh-oh-oh-oh, oh-oh-oh-oh, you, have, got, som...     0.810657   5.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "#Get all relevant docs from boolean retrival\n",
    "query = search_queries[0]\n",
    "result = index.search(query)\n",
    "\n",
    "#Gives only those docs which are relevant\n",
    "filtered_df = df[df['DocumentID'].isin(result)]\n",
    "filtered_df['Query_Score'] = 0\n",
    "for term in query.split():\n",
    "    idf_score = float(idf(term, df))\n",
    "    filtered_df['Query_Score'] += filtered_df.apply(lambda row: term_frequency(row['Tokens'], term), axis=1)*idf_score\n",
    "filtered_df = filtered_df.sort_values(by=\"Query_Score\", ascending=False).reset_index(drop=True)\n",
    "filtered_df['Rank'] = filtered_df['Query_Score'].rank(ascending=False)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOpA6LgFkQ1S"
   },
   "source": [
    "Now show the results for the query: `never know`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TLik1D0VkQ1S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Query_Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skrillex-pretty-bye-bye-lyrics</td>\n",
       "      <td>i know you are trouble but i can't seem to ...</td>\n",
       "      <td>[i, know, you, are, trouble, but, i, can, not,...</td>\n",
       "      <td>1.295128</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Derek-minor-until-the-end-of-time-lyrics</td>\n",
       "      <td>time is the fourth dimension and a measure ...</td>\n",
       "      <td>[time, is, the, fourth, dimension, and, a, mea...</td>\n",
       "      <td>1.145859</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billie-eilish-8-lyrics</td>\n",
       "      <td>wait a minute, let me finish i know you don...</td>\n",
       "      <td>[wait, a, minute, ,, let, me, finish, i, know,...</td>\n",
       "      <td>1.135649</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bring-me-the-horizon-mother-tongue-lyrics</td>\n",
       "      <td>i didn't see it coming  but i never really ...</td>\n",
       "      <td>[i, did, not, see, it, coming, but, i, never, ...</td>\n",
       "      <td>1.094982</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marilyn-manson-coma-black-lyrics</td>\n",
       "      <td>part i: eden eye   \"a loved one laid his hea...</td>\n",
       "      <td>[part, i, :, eden, eye, ``, a, loved, one, lai...</td>\n",
       "      <td>1.093421</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  DocumentID  \\\n",
       "0             Skrillex-pretty-bye-bye-lyrics   \n",
       "1   Derek-minor-until-the-end-of-time-lyrics   \n",
       "2                     Billie-eilish-8-lyrics   \n",
       "3  Bring-me-the-horizon-mother-tongue-lyrics   \n",
       "4           Marilyn-manson-coma-black-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     i know you are trouble but i can't seem to ...   \n",
       "1     time is the fourth dimension and a measure ...   \n",
       "2     wait a minute, let me finish i know you don...   \n",
       "3     i didn't see it coming  but i never really ...   \n",
       "4    part i: eden eye   \"a loved one laid his hea...   \n",
       "\n",
       "                                              Tokens  Query_Score  Rank  \n",
       "0  [i, know, you, are, trouble, but, i, can, not,...     1.295128   1.0  \n",
       "1  [time, is, the, fourth, dimension, and, a, mea...     1.145859   2.0  \n",
       "2  [wait, a, minute, ,, let, me, finish, i, know,...     1.135649   3.0  \n",
       "3  [i, did, not, see, it, coming, but, i, never, ...     1.094982   4.0  \n",
       "4  [part, i, :, eden, eye, ``, a, loved, one, lai...     1.093421   5.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "query = search_queries[1]\n",
    "result = index.search(query)\n",
    "\n",
    "#Gives only those docs which are relevant\n",
    "filtered_df = df[df['DocumentID'].isin(result)]\n",
    "filtered_df['Query_Score'] = 0\n",
    "\n",
    "for term in query.split():\n",
    "    idf_score = float(idf(term, df))\n",
    "    filtered_df['Query_Score'] += filtered_df.apply(lambda row: term_frequency(row['Tokens'], term), axis=1)*idf_score\n",
    "filtered_df = filtered_df.sort_values(by=\"Query_Score\", ascending=False).reset_index(drop=True)\n",
    "filtered_df['Rank'] = filtered_df['Query_Score'].rank(ascending=False)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksB32YbUkQ1S"
   },
   "source": [
    "Now show the results for the query: `make no sense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "g81t_UrMkQ1S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Query_Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bring-me-the-horizon-mother-tongue-lyrics</td>\n",
       "      <td>i didn't see it coming  but i never really ...</td>\n",
       "      <td>[i, did, not, see, it, coming, but, i, never, ...</td>\n",
       "      <td>4.482244</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Florence-the-machine-all-this-and-heaven-too-l...</td>\n",
       "      <td>and the heart is hard to translate it has a...</td>\n",
       "      <td>[and, the, heart, is, hard, to, translate, it,...</td>\n",
       "      <td>3.278860</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kid-cudi-ghost-lyrics</td>\n",
       "      <td>yeah, woah-woah, oh yeah, woah-woah, haha y...</td>\n",
       "      <td>[yeah, ,, woah-woah, ,, oh, yeah, ,, woah-woah...</td>\n",
       "      <td>3.085164</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linkin-park-one-step-closer-lyrics</td>\n",
       "      <td>i cannot take this anymore saying everythin...</td>\n",
       "      <td>[i, can, not, take, this, anymore, saying, eve...</td>\n",
       "      <td>3.050704</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Devlin-all-along-the-watchtower-lyrics</td>\n",
       "      <td>there must be some way out of here said the...</td>\n",
       "      <td>[there, must, be, some, way, out, of, here, sa...</td>\n",
       "      <td>2.680610</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          DocumentID  \\\n",
       "0          Bring-me-the-horizon-mother-tongue-lyrics   \n",
       "1  Florence-the-machine-all-this-and-heaven-too-l...   \n",
       "2                              Kid-cudi-ghost-lyrics   \n",
       "3                 Linkin-park-one-step-closer-lyrics   \n",
       "4             Devlin-all-along-the-watchtower-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     i didn't see it coming  but i never really ...   \n",
       "1     and the heart is hard to translate it has a...   \n",
       "2     yeah, woah-woah, oh yeah, woah-woah, haha y...   \n",
       "3     i cannot take this anymore saying everythin...   \n",
       "4     there must be some way out of here said the...   \n",
       "\n",
       "                                              Tokens  Query_Score  Rank  \n",
       "0  [i, did, not, see, it, coming, but, i, never, ...     4.482244   1.0  \n",
       "1  [and, the, heart, is, hard, to, translate, it,...     3.278860   2.0  \n",
       "2  [yeah, ,, woah-woah, ,, oh, yeah, ,, woah-woah...     3.085164   3.0  \n",
       "3  [i, can, not, take, this, anymore, saying, eve...     3.050704   4.0  \n",
       "4  [there, must, be, some, way, out, of, here, sa...     2.680610   5.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "query = search_queries[2]\n",
    "result = index.search(query)\n",
    "\n",
    "#Gives only those docs which are relevant\n",
    "filtered_df = df[df['DocumentID'].isin(result)]\n",
    "filtered_df['Query_Score'] = 0\n",
    "for term in query.split():\n",
    "    idf_score = float(idf(term, df))\n",
    "    filtered_df['Query_Score'] += filtered_df.apply(lambda row: term_frequency(row['Tokens'], term), axis=1)*idf_score\n",
    "filtered_df = filtered_df.sort_values(by=\"Query_Score\", ascending=False).reset_index(drop=True)\n",
    "filtered_df['Rank'] = filtered_df['Query_Score'].rank(ascending=False)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Jyzt0qwN-Js"
   },
   "source": [
    "### B: Ranking with vector space model with TF-IDF (10 points) \n",
    "\n",
    "**Cosine:** You should use cosine as your scoring function. \n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TF-IDF scores as introduced in A. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: never $\\rightarrow$ (1)\n",
    "* query: never know $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "You should output the top-5 results plus the cosine score of each of these documents.  \n",
    "\n",
    "The output should be like this:\n",
    "\n",
    "Rank Scores DocumentID Document \n",
    "\n",
    "---\n",
    "\n",
    "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_all_terms(df, doc_dict):\n",
    "    idf_score = []\n",
    "    for term in doc_dict:\n",
    "        idf_score.append(idf(term, df))\n",
    "    return idf_score\n",
    "\n",
    "def doc_tf_idf(df, doc_dict):\n",
    "    idf_score = idf_all_terms(df, doc_dict)\n",
    "    df['tf_idf'] = [[] for _ in range(len(df))]\n",
    "    for i in range(len(df)):\n",
    "        scores = [0 for i in range(0,len(doc_dict))]\n",
    "        freq = [0 for i in range(0,len(doc_dict))]\n",
    "        \n",
    "        # Update Frequency\n",
    "        for word in df['Tokens'][i]:\n",
    "            index = doc_dict.index(word) \n",
    "            freq[index] += 1\n",
    "            \n",
    "        # Update Scores\n",
    "        for word in df['Tokens'][i]:\n",
    "            index = doc_dict.index(word) \n",
    "            scores[index] = (1 + (math.log(freq[index])/math.log(10)))*idf_score[index]\n",
    "        \n",
    "        normalized_value = math.sqrt(sum([score**2 for score in scores]))\n",
    "        scores = [score/normalized_value for score in scores]\n",
    "        \n",
    "        # Update the dataframe\n",
    "        df['tf_idf'][i] = scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tf_idf(df, doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index and Normalize Query\n",
    "def index_norm_query(text, doc_dict):\n",
    "    terms = text.split()\n",
    "    query_score = [0 for i in range(0,len(doc_dict))]\n",
    "    for term in terms:\n",
    "        index = doc_dict.index(term)\n",
    "        query_score[index] += 1\n",
    "    normalized_value = math.sqrt(sum([score**2 for score in query_score]))\n",
    "    query_score = [score/normalized_value for score in query_score]\n",
    "    return query_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlZvcIPCjV5O"
   },
   "source": [
    "Now show the results for the query: `time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "4_Nk12slN-Js"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John-legend-for-the-first-time-lyrics</td>\n",
       "      <td>is that music in my ear it's like i heard...</td>\n",
       "      <td>[is, that, music, in, my, ear, it, is, like, i...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.101289</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The-beatles-carry-that-weight-lyrics</td>\n",
       "      <td>boy, you're gonna carry that weight carry t...</td>\n",
       "      <td>[boy, ,, you, are, going, to, carry, that, wei...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.096126</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The-beatles-ill-follow-the-sun-lyrics</td>\n",
       "      <td>one day, you'll look to see i've gone for t...</td>\n",
       "      <td>[one, day, ,, you, will, look, to, see, i, hav...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.076795</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lana-del-rey-burnt-norton-interlude-lyrics</td>\n",
       "      <td>time present and time past are both perhaps...</td>\n",
       "      <td>[time, present, and, time, past, are, both, pe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.073668</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linkin-park-from-the-inside-lyrics</td>\n",
       "      <td>i don't know who to trust, no surprise ever...</td>\n",
       "      <td>[i, do, not, know, who, to, trust, ,, no, surp...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.071755</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   DocumentID  \\\n",
       "0       John-legend-for-the-first-time-lyrics   \n",
       "1        The-beatles-carry-that-weight-lyrics   \n",
       "2       The-beatles-ill-follow-the-sun-lyrics   \n",
       "3  Lana-del-rey-burnt-norton-interlude-lyrics   \n",
       "4          Linkin-park-from-the-inside-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0       is that music in my ear it's like i heard...   \n",
       "1     boy, you're gonna carry that weight carry t...   \n",
       "2     one day, you'll look to see i've gone for t...   \n",
       "3     time present and time past are both perhaps...   \n",
       "4     i don't know who to trust, no surprise ever...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [is, that, music, in, my, ear, it, is, like, i...   \n",
       "1  [boy, ,, you, are, going, to, carry, that, wei...   \n",
       "2  [one, day, ,, you, will, look, to, see, i, hav...   \n",
       "3  [time, present, and, time, past, are, both, pe...   \n",
       "4  [i, do, not, know, who, to, trust, ,, no, surp...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.101289   1.0  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.096126   2.0  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.076795   3.0  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.073668   4.0  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.071755   5.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "query_vector = index_norm_query(search_queries[0], doc_dict)\n",
    "df['Score'] = [sum([i*j for i,j in zip(row, query_vector)]) for row in df['tf_idf']]\n",
    "df = df.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "df['Rank'] = df['Score'].rank(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDAyxTU_jj-p"
   },
   "source": [
    "Now show the results for the query: `never know`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QDhuNQ8SjkYU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skrillex-pretty-bye-bye-lyrics</td>\n",
       "      <td>i know you are trouble but i can't seem to ...</td>\n",
       "      <td>[i, know, you, are, trouble, but, i, can, not,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.176...</td>\n",
       "      <td>0.073648</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John-legend-made-to-love-lyrics</td>\n",
       "      <td>i was sent here for you we were made to l...</td>\n",
       "      <td>[i, wa, sent, here, for, you, we, were, made, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.073598</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billie-eilish-8-lyrics</td>\n",
       "      <td>wait a minute, let me finish i know you don...</td>\n",
       "      <td>[wait, a, minute, ,, let, me, finish, i, know,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.06911137531833073, 0.0602141...</td>\n",
       "      <td>0.068107</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Panic-at-the-disco-girls-girls-boys-lyrics</td>\n",
       "      <td>i don't want to hear you got a boyfriend so...</td>\n",
       "      <td>[i, do, not, want, to, hear, you, got, a, boyf...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.063735</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Muse-aftermath-lyrics</td>\n",
       "      <td>war is all around i'm growing tired of figh...</td>\n",
       "      <td>[war, is, all, around, i, am, growing, tired, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.059311</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   DocumentID  \\\n",
       "0              Skrillex-pretty-bye-bye-lyrics   \n",
       "1             John-legend-made-to-love-lyrics   \n",
       "2                      Billie-eilish-8-lyrics   \n",
       "3  Panic-at-the-disco-girls-girls-boys-lyrics   \n",
       "4                       Muse-aftermath-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     i know you are trouble but i can't seem to ...   \n",
       "1       i was sent here for you we were made to l...   \n",
       "2     wait a minute, let me finish i know you don...   \n",
       "3     i don't want to hear you got a boyfriend so...   \n",
       "4     war is all around i'm growing tired of figh...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [i, know, you, are, trouble, but, i, can, not,...   \n",
       "1  [i, wa, sent, here, for, you, we, were, made, ...   \n",
       "2  [wait, a, minute, ,, let, me, finish, i, know,...   \n",
       "3  [i, do, not, want, to, hear, you, got, a, boyf...   \n",
       "4  [war, is, all, around, i, am, growing, tired, ...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.176...  0.073648   1.0  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.073598   2.0  \n",
       "2  [0.0, 0.0, 0.0, 0.06911137531833073, 0.0602141...  0.068107   3.0  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.063735   4.0  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.059311   5.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "query_vector = index_norm_query(search_queries[1], doc_dict)\n",
    "df['Score'] = [sum([i*j for i,j in zip(row, query_vector)]) for row in df['tf_idf']]\n",
    "df = df.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "df['Rank'] = df['Score'].rank(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "detrTVmMjk3I"
   },
   "source": [
    "Now show the results for the query: `make no sense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Ct3N8xrsjlbn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linkin-park-one-step-closer-lyrics</td>\n",
       "      <td>i cannot take this anymore saying everythin...</td>\n",
       "      <td>[i, can, not, take, this, anymore, saying, eve...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.185360</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bring-me-the-horizon-mother-tongue-lyrics</td>\n",
       "      <td>i didn't see it coming  but i never really ...</td>\n",
       "      <td>[i, did, not, see, it, coming, but, i, never, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.08485694890580205, 0.0,...</td>\n",
       "      <td>0.167635</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shawn-mendes-act-like-you-love-me-lyrics</td>\n",
       "      <td>so you leave tomorrow, just sleep the night...</td>\n",
       "      <td>[so, you, leave, tomorrow, ,, just, sleep, the...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.120528</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Florence-the-machine-all-this-and-heaven-too-l...</td>\n",
       "      <td>and the heart is hard to translate it has a...</td>\n",
       "      <td>[and, the, heart, is, hard, to, translate, it,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.118449</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kid-cudi-ghost-lyrics</td>\n",
       "      <td>yeah, woah-woah, oh yeah, woah-woah, haha y...</td>\n",
       "      <td>[yeah, ,, woah-woah, ,, oh, yeah, ,, woah-woah...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.046603625175871494, 0.0, 0.0...</td>\n",
       "      <td>0.101871</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          DocumentID  \\\n",
       "0                 Linkin-park-one-step-closer-lyrics   \n",
       "1          Bring-me-the-horizon-mother-tongue-lyrics   \n",
       "2           Shawn-mendes-act-like-you-love-me-lyrics   \n",
       "3  Florence-the-machine-all-this-and-heaven-too-l...   \n",
       "4                              Kid-cudi-ghost-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     i cannot take this anymore saying everythin...   \n",
       "1     i didn't see it coming  but i never really ...   \n",
       "2     so you leave tomorrow, just sleep the night...   \n",
       "3     and the heart is hard to translate it has a...   \n",
       "4     yeah, woah-woah, oh yeah, woah-woah, haha y...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [i, can, not, take, this, anymore, saying, eve...   \n",
       "1  [i, did, not, see, it, coming, but, i, never, ...   \n",
       "2  [so, you, leave, tomorrow, ,, just, sleep, the...   \n",
       "3  [and, the, heart, is, hard, to, translate, it,...   \n",
       "4  [yeah, ,, woah-woah, ,, oh, yeah, ,, woah-woah...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.185360   1.0  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.08485694890580205, 0.0,...  0.167635   2.0  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.120528   3.0  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.118449   4.0  \n",
       "4  [0.0, 0.0, 0.0, 0.046603625175871494, 0.0, 0.0...  0.101871   5.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "query_vector = index_norm_query(search_queries[2], doc_dict)\n",
    "df['Score'] = [sum([i*j for i,j in zip(row, query_vector)]) for row in df['tf_idf']]\n",
    "df = df.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "df['Rank'] = df['Score'].rank(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C82_e_CgN-Jt"
   },
   "source": [
    "### C: Ranking with BM25 (10 points) \n",
    "Finally, let's try the BM25 approach for ranking. Refer to https://en.wikipedia.org/wiki/Okapi_BM25 for the specific formula. You could choose k_1 = 1.2 and b = 0.75 but feel free to try other options.\n",
    "\n",
    "**Output:**\n",
    "You should output the top-5 results plus the BM25 score of each of these documents.  \n",
    "\n",
    "The output should be like this:\n",
    "\n",
    "Rank Scores DocumentID Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate BM25\n",
    "def bm25_score(df, column, query, k1=1.2, b=0.75):\n",
    "    N = len(df)\n",
    "    avg_len = sum(df[column].apply(len)) / N\n",
    "    scores = []\n",
    "    for i in range(N):\n",
    "        document = df[column].iloc[i]\n",
    "        score = 0\n",
    "        for word in query.split():\n",
    "            n = sum(1 for d in df[column] if word in d)\n",
    "            idf = math.log(N/n)/math.log(10)\n",
    "            f = document.count(word)\n",
    "            score += idf * (f * (k1 + 1) / (f + k1 * (1 - b + b * len(document) / avg_len)))\n",
    "        scores.append(score)\n",
    "    return pd.Series(scores, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P1o4f7Ik2tS"
   },
   "source": [
    "Now show the results for the query: `time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "M0TWNgwYk2tU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John-legend-for-the-first-time-lyrics</td>\n",
       "      <td>is that music in my ear it's like i heard...</td>\n",
       "      <td>[is, that, music, in, my, ear, it, is, like, i...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.880378</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lana-del-rey-burnt-norton-interlude-lyrics</td>\n",
       "      <td>time present and time past are both perhaps...</td>\n",
       "      <td>[time, present, and, time, past, are, both, pe...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.873321</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sza-pray-lyrics</td>\n",
       "      <td>pray for yourself one time one time, time ...</td>\n",
       "      <td>[pray, for, yourself, one, time, one, time, ,,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.849563</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>System-of-a-down-thetawaves-lyrics</td>\n",
       "      <td>the unsettled mind is at times an ally leav...</td>\n",
       "      <td>[the, unsettled, mind, is, at, time, an, ally,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.847497</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linkin-park-from-the-inside-lyrics</td>\n",
       "      <td>i don't know who to trust, no surprise ever...</td>\n",
       "      <td>[i, do, not, know, who, to, trust, ,, no, surp...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.834475</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   DocumentID  \\\n",
       "0       John-legend-for-the-first-time-lyrics   \n",
       "1  Lana-del-rey-burnt-norton-interlude-lyrics   \n",
       "2                             Sza-pray-lyrics   \n",
       "3          System-of-a-down-thetawaves-lyrics   \n",
       "4          Linkin-park-from-the-inside-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0       is that music in my ear it's like i heard...   \n",
       "1     time present and time past are both perhaps...   \n",
       "2      pray for yourself one time one time, time ...   \n",
       "3     the unsettled mind is at times an ally leav...   \n",
       "4     i don't know who to trust, no surprise ever...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [is, that, music, in, my, ear, it, is, like, i...   \n",
       "1  [time, present, and, time, past, are, both, pe...   \n",
       "2  [pray, for, yourself, one, time, one, time, ,,...   \n",
       "3  [the, unsettled, mind, is, at, time, an, ally,...   \n",
       "4  [i, do, not, know, who, to, trust, ,, no, surp...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.880378   1.0  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.873321   2.0  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.849563   3.0  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.847497   4.0  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.834475   5.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "df['Score'] = bm25_score(df, 'Tokens', search_queries[0], 1.2, 0.75)\n",
    "df = df.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "df['Rank'] = df['Score'].rank(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbwIhabwk2tU"
   },
   "source": [
    "Now show the results for the query: `never know`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vW6AnSdJk2tV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skrillex-pretty-bye-bye-lyrics</td>\n",
       "      <td>i know you are trouble but i can't seem to ...</td>\n",
       "      <td>[i, know, you, are, trouble, but, i, can, not,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.176...</td>\n",
       "      <td>1.264629</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Billie-eilish-8-lyrics</td>\n",
       "      <td>wait a minute, let me finish i know you don...</td>\n",
       "      <td>[wait, a, minute, ,, let, me, finish, i, know,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.06911137531833073, 0.0602141...</td>\n",
       "      <td>1.249621</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Panic-at-the-disco-girls-girls-boys-lyrics</td>\n",
       "      <td>i don't want to hear you got a boyfriend so...</td>\n",
       "      <td>[i, do, not, want, to, hear, you, got, a, boyf...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.169681</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marina-the-archetypes-lyrics</td>\n",
       "      <td>housewife, beauty queen, homewrecker, idle t...</td>\n",
       "      <td>[housewife, ,, beauty, queen, ,, homewrecker, ...</td>\n",
       "      <td>[0.0, 0.0, 0.15988212494129256, 0.0, 0.0622348...</td>\n",
       "      <td>1.137158</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alessia-cara-outlaws-lyrics</td>\n",
       "      <td>run, run like the devils behind us run to ...</td>\n",
       "      <td>[run, ,, run, like, the, devil, , s, behind, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.03263832494120442, 0.0, 0.0,...</td>\n",
       "      <td>1.134210</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   DocumentID  \\\n",
       "0              Skrillex-pretty-bye-bye-lyrics   \n",
       "1                      Billie-eilish-8-lyrics   \n",
       "2  Panic-at-the-disco-girls-girls-boys-lyrics   \n",
       "3                Marina-the-archetypes-lyrics   \n",
       "4                 Alessia-cara-outlaws-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     i know you are trouble but i can't seem to ...   \n",
       "1     wait a minute, let me finish i know you don...   \n",
       "2     i don't want to hear you got a boyfriend so...   \n",
       "3    housewife, beauty queen, homewrecker, idle t...   \n",
       "4     run, run like the devils behind us run to ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [i, know, you, are, trouble, but, i, can, not,...   \n",
       "1  [wait, a, minute, ,, let, me, finish, i, know,...   \n",
       "2  [i, do, not, want, to, hear, you, got, a, boyf...   \n",
       "3  [housewife, ,, beauty, queen, ,, homewrecker, ...   \n",
       "4  [run, ,, run, like, the, devil, , s, behind, ...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.176...  1.264629   1.0  \n",
       "1  [0.0, 0.0, 0.0, 0.06911137531833073, 0.0602141...  1.249621   2.0  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.169681   3.0  \n",
       "3  [0.0, 0.0, 0.15988212494129256, 0.0, 0.0622348...  1.137158   4.0  \n",
       "4  [0.0, 0.0, 0.0, 0.03263832494120442, 0.0, 0.0,...  1.134210   5.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "df['Score'] = bm25_score(df, 'Tokens', search_queries[1], 1.2, 0.75)\n",
    "df = df.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "df['Rank'] = df['Score'].rank(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgT5flFMk2tZ"
   },
   "source": [
    "Now show the results for the query: `make no sense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4BfNnVvWk2tZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bring-me-the-horizon-mother-tongue-lyrics</td>\n",
       "      <td>i didn't see it coming  but i never really ...</td>\n",
       "      <td>[i, did, not, see, it, coming, but, i, never, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.08485694890580205, 0.0,...</td>\n",
       "      <td>4.327209</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linkin-park-one-step-closer-lyrics</td>\n",
       "      <td>i cannot take this anymore saying everythin...</td>\n",
       "      <td>[i, can, not, take, this, anymore, saying, eve...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3.758132</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florence-the-machine-all-this-and-heaven-too-l...</td>\n",
       "      <td>and the heart is hard to translate it has a...</td>\n",
       "      <td>[and, the, heart, is, hard, to, translate, it,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>3.483398</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kid-cudi-ghost-lyrics</td>\n",
       "      <td>yeah, woah-woah, oh yeah, woah-woah, haha y...</td>\n",
       "      <td>[yeah, ,, woah-woah, ,, oh, yeah, ,, woah-woah...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.046603625175871494, 0.0, 0.0...</td>\n",
       "      <td>3.056984</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shawn-mendes-act-like-you-love-me-lyrics</td>\n",
       "      <td>so you leave tomorrow, just sleep the night...</td>\n",
       "      <td>[so, you, leave, tomorrow, ,, just, sleep, the...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2.355055</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          DocumentID  \\\n",
       "0          Bring-me-the-horizon-mother-tongue-lyrics   \n",
       "1                 Linkin-park-one-step-closer-lyrics   \n",
       "2  Florence-the-machine-all-this-and-heaven-too-l...   \n",
       "3                              Kid-cudi-ghost-lyrics   \n",
       "4           Shawn-mendes-act-like-you-love-me-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     i didn't see it coming  but i never really ...   \n",
       "1     i cannot take this anymore saying everythin...   \n",
       "2     and the heart is hard to translate it has a...   \n",
       "3     yeah, woah-woah, oh yeah, woah-woah, haha y...   \n",
       "4     so you leave tomorrow, just sleep the night...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [i, did, not, see, it, coming, but, i, never, ...   \n",
       "1  [i, can, not, take, this, anymore, saying, eve...   \n",
       "2  [and, the, heart, is, hard, to, translate, it,...   \n",
       "3  [yeah, ,, woah-woah, ,, oh, yeah, ,, woah-woah...   \n",
       "4  [so, you, leave, tomorrow, ,, just, sleep, the...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.08485694890580205, 0.0,...  4.327209   1.0  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  3.758132   2.0  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  3.483398   3.0  \n",
       "3  [0.0, 0.0, 0.0, 0.046603625175871494, 0.0, 0.0...  3.056984   4.0  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  2.355055   5.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "df['Score'] = bm25_score(df, 'Tokens', search_queries[2], 1.2, 0.75)\n",
    "df = df.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "df['Rank'] = df['Score'].rank(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOTZbC3_N-Jt"
   },
   "source": [
    "### Discussion (10 points)\n",
    "Briefly discuss the differences you see between the three methods. You should try additional queries beyond the ones we list. Is there a ranking approach you prefer? Explain and give concrete examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY tested: \"will you be there\" answer expected: \"Michael-jackson-will-you-be-there-lyrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"will you be there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Query_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frank-ocean-godspeed-dvsn-remix-lyrics</td>\n",
       "      <td>i will always love you how i do let go of a...</td>\n",
       "      <td>[i, will, always, love, you, how, i, do, let, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.951193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rihanna-umbrella-lyrics</td>\n",
       "      <td>uh-huh, uh-huh  uh-huh, uh-huh  uh-huh, uh-...</td>\n",
       "      <td>[uh-huh, ,, uh-huh, uh-huh, ,, uh-huh, uh-huh,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.052764938707345325, 0.0, 0.0...</td>\n",
       "      <td>0.435229</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.856156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skylar-grey-tower-dont-look-down-lyrics</td>\n",
       "      <td>you're high up on the tower now dont look ...</td>\n",
       "      <td>[you, are, high, up, on, the, tower, now, do, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.05436794727557258, 0.0, 0.0,...</td>\n",
       "      <td>1.396168</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.836598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lin-manuel-miranda-what-the-heck-i-gotta-do-ly...</td>\n",
       "      <td>the plan was called operation d-minus. and ...</td>\n",
       "      <td>[the, plan, wa, called, operation, d-minus, .,...</td>\n",
       "      <td>[0.08943780129527809, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.669664</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.835006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kanye-west-low-lights-lyrics</td>\n",
       "      <td>you want me to give you a testimony about m...</td>\n",
       "      <td>[you, want, me, to, give, you, a, testimony, a...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.101982</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.828740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          DocumentID  \\\n",
       "0             Frank-ocean-godspeed-dvsn-remix-lyrics   \n",
       "1                            Rihanna-umbrella-lyrics   \n",
       "2            Skylar-grey-tower-dont-look-down-lyrics   \n",
       "3  Lin-manuel-miranda-what-the-heck-i-gotta-do-ly...   \n",
       "4                       Kanye-west-low-lights-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     i will always love you how i do let go of a...   \n",
       "1     uh-huh, uh-huh  uh-huh, uh-huh  uh-huh, uh-...   \n",
       "2     you're high up on the tower now dont look ...   \n",
       "3     the plan was called operation d-minus. and ...   \n",
       "4     you want me to give you a testimony about m...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [i, will, always, love, you, how, i, do, let, ...   \n",
       "1  [uh-huh, ,, uh-huh, uh-huh, ,, uh-huh, uh-huh,...   \n",
       "2  [you, are, high, up, on, the, tower, now, do, ...   \n",
       "3  [the, plan, wa, called, operation, d-minus, .,...   \n",
       "4  [you, want, me, to, give, you, a, testimony, a...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.000000   1.0   \n",
       "1  [0.0, 0.0, 0.0, 0.052764938707345325, 0.0, 0.0...  0.435229   2.0   \n",
       "2  [0.0, 0.0, 0.0, 0.05436794727557258, 0.0, 0.0,...  1.396168   3.0   \n",
       "3  [0.08943780129527809, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.669664   4.0   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.101982   5.0   \n",
       "\n",
       "   Query_Score  \n",
       "0     1.951193  \n",
       "1     1.856156  \n",
       "2     1.836598  \n",
       "3     1.835006  \n",
       "4     1.828740  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "result = index.search(query)\n",
    "\n",
    "#Gives only those docs which are relevant\n",
    "filtered_df = df[df['DocumentID'].isin(result)]\n",
    "filtered_df['Query_Score'] = 0\n",
    "\n",
    "for term in query.split():\n",
    "    idf_score = float(idf(term, df))\n",
    "    filtered_df['Query_Score'] += filtered_df.apply(lambda row: term_frequency(row['Tokens'], term), axis=1)*idf_score\n",
    "filtered_df = filtered_df.sort_values(by=\"Query_Score\", ascending=False).reset_index(drop=True)\n",
    "filtered_df['Rank'] = filtered_df['Query_Score'].rank(ascending=False)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kanye-west-low-lights-lyrics</td>\n",
       "      <td>you want me to give you a testimony about m...</td>\n",
       "      <td>[you, want, me, to, give, you, a, testimony, a...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.084153</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imagine-dragons-release-lyrics</td>\n",
       "      <td>twenty miles from anyone set my sights on t...</td>\n",
       "      <td>[twenty, mile, from, anyone, set, my, sight, o...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.083514</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alicia-keys-no-one-lyrics</td>\n",
       "      <td>i just want you close where you can stay fo...</td>\n",
       "      <td>[i, just, want, you, close, where, you, can, s...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.08539408657035345, 0.0, 0.0,...</td>\n",
       "      <td>0.082507</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Frank-ocean-godspeed-dvsn-remix-lyrics</td>\n",
       "      <td>i will always love you how i do let go of a...</td>\n",
       "      <td>[i, will, always, love, you, how, i, do, let, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.079368</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skylar-grey-tower-dont-look-down-lyrics</td>\n",
       "      <td>you're high up on the tower now dont look ...</td>\n",
       "      <td>[you, are, high, up, on, the, tower, now, do, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.05436794727557258, 0.0, 0.0,...</td>\n",
       "      <td>0.075445</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                DocumentID  \\\n",
       "0             Kanye-west-low-lights-lyrics   \n",
       "1           Imagine-dragons-release-lyrics   \n",
       "2                Alicia-keys-no-one-lyrics   \n",
       "3   Frank-ocean-godspeed-dvsn-remix-lyrics   \n",
       "4  Skylar-grey-tower-dont-look-down-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     you want me to give you a testimony about m...   \n",
       "1     twenty miles from anyone set my sights on t...   \n",
       "2     i just want you close where you can stay fo...   \n",
       "3     i will always love you how i do let go of a...   \n",
       "4     you're high up on the tower now dont look ...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [you, want, me, to, give, you, a, testimony, a...   \n",
       "1  [twenty, mile, from, anyone, set, my, sight, o...   \n",
       "2  [i, just, want, you, close, where, you, can, s...   \n",
       "3  [i, will, always, love, you, how, i, do, let, ...   \n",
       "4  [you, are, high, up, on, the, tower, now, do, ...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.084153   1.0  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.083514   2.0  \n",
       "2  [0.0, 0.0, 0.0, 0.08539408657035345, 0.0, 0.0,...  0.082507   3.0  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.079368   4.0  \n",
       "4  [0.0, 0.0, 0.0, 0.05436794727557258, 0.0, 0.0,...  0.075445   5.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VectorSpace with TF-IDF\n",
    "query_vector = index_norm_query(query, doc_dict)\n",
    "df['Score'] = [sum([i*j for i,j in zip(row, query_vector)]) for row in df['tf_idf']]\n",
    "df = df.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "df['Rank'] = df['Score'].rank(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocumentID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>Score</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frank-ocean-godspeed-dvsn-remix-lyrics</td>\n",
       "      <td>i will always love you how i do let go of a...</td>\n",
       "      <td>[i, will, always, love, you, how, i, do, let, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2.012258</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michael-jackson-will-you-be-there-lyrics</td>\n",
       "      <td>hold me like the river jordan and i will th...</td>\n",
       "      <td>[hold, me, like, the, river, jordan, and, i, w...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2.005938</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kanye-west-low-lights-lyrics</td>\n",
       "      <td>you want me to give you a testimony about m...</td>\n",
       "      <td>[you, want, me, to, give, you, a, testimony, a...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.934673</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skylar-grey-tower-dont-look-down-lyrics</td>\n",
       "      <td>you're high up on the tower now dont look ...</td>\n",
       "      <td>[you, are, high, up, on, the, tower, now, do, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.05436794727557258, 0.0, 0.0,...</td>\n",
       "      <td>1.930400</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Miley-cyrus-if-we-were-a-movie-lyrics</td>\n",
       "      <td>uh oh there you go again, talking cinematic...</td>\n",
       "      <td>[uh, oh, there, you, go, again, ,, talking, ci...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.844927</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 DocumentID  \\\n",
       "0    Frank-ocean-godspeed-dvsn-remix-lyrics   \n",
       "1  Michael-jackson-will-you-be-there-lyrics   \n",
       "2              Kanye-west-low-lights-lyrics   \n",
       "3   Skylar-grey-tower-dont-look-down-lyrics   \n",
       "4     Miley-cyrus-if-we-were-a-movie-lyrics   \n",
       "\n",
       "                                            Document  \\\n",
       "0     i will always love you how i do let go of a...   \n",
       "1     hold me like the river jordan and i will th...   \n",
       "2     you want me to give you a testimony about m...   \n",
       "3     you're high up on the tower now dont look ...   \n",
       "4     uh oh there you go again, talking cinematic...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [i, will, always, love, you, how, i, do, let, ...   \n",
       "1  [hold, me, like, the, river, jordan, and, i, w...   \n",
       "2  [you, want, me, to, give, you, a, testimony, a...   \n",
       "3  [you, are, high, up, on, the, tower, now, do, ...   \n",
       "4  [uh, oh, there, you, go, again, ,, talking, ci...   \n",
       "\n",
       "                                              tf_idf     Score  Rank  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  2.012258   1.0  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  2.005938   2.0  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.934673   3.0  \n",
       "3  [0.0, 0.0, 0.0, 0.05436794727557258, 0.0, 0.0,...  1.930400   4.0  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.844927   5.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BM-25\n",
    "df['Score'] = bm25_score(df, 'Tokens', query, 1.2, 0.75)\n",
    "df = df.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "df['Rank'] = df['Score'].rank(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ed1RYi-P0VCM"
   },
   "source": [
    "### The advantages and disadvantages of each of the algorithms tried:\n",
    "\n",
    "Using the TF-IDF algorithm, lyrics that are more pertinent to the search query are given a better ranking. This algorithm considers the term frequency (TF) and inverse document frequency (IDF) of the words in the lyrics. However, because it does not use the lyrics' contextual information, it might not perform well in situations when the context is crucial for the search results.\n",
    "\n",
    "Using TF-IDF to calculate the cosine similarity of the vectors that represent the query and the lyrics, this technique determines how similar the query and the lyrics are to one another. When looking for lyrics that are semantically related to a query, this approach to song lyric search might be extremely useful.\n",
    "\n",
    "BM25: Similar to TF-IDF, BM25 is a probabilistic model that determines the relevance score between the search term and the lyrics while accounting for TF and IDF as well as other factors. Because it provides a more comprehensive approach to addressing term frequency and inverse document frequency, BM25 may occasionally be more effective than TF-IDF.\n",
    "\n",
    "### Preferred Ranking Approach\n",
    "\n",
    "As discussed advantages and disadvantages above, and looking at the results for given queries, BM25 algorithm worked better. \n",
    "I tried with custom query where answer is expected to be 'Michael-jackson-will-you-be-there-lyrics', only BM25 returned the song in top 5 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYce9oRO6sbl"
   },
   "source": [
    "# Part 4: Cool Extension (10 points) \n",
    "\n",
    "Finally, we give you an opportunity to explore some more sophisticated approach to your search engine. This is your chance to show off something you find interesting. For example, you might:\n",
    "\n",
    "\n",
    "*   Add a positional index so you can support phrase queries\n",
    "*   Implement a permuterm index for wildcard queries\n",
    "*   Incorporate spell correction\n",
    "*   Index all of the lyrics at https://www.cs.cornell.edu/~arb/data/genius-expertise/ and demonstrate an efficient implementation\n",
    "*   Try a more advanced ranking approach\n",
    "*   ...\n",
    "\n",
    "We will grade this last part according to effort, creativity, and impact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Query is: will you be there \n",
      "Corrected Query is: hello world \n",
      "Corrected Query is: time \n",
      "Corrected Query is: love \n",
      "Corrected Query is: never know \n",
      "Corrected Query is: make no sense \n"
     ]
    }
   ],
   "source": [
    "# Spell Correction\n",
    "def correct_spellings(word, dictionary):\n",
    "    word = word.lower()\n",
    "    closest_word = word\n",
    "    closest_distance = float('inf')\n",
    "    for dict_word in dictionary:\n",
    "        distance = edit_distance(word, dict_word)\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_word = dict_word\n",
    "    return closest_word\n",
    "\n",
    "def edit_distance(word1, word2):\n",
    "    m, n = len(word1), len(word2)\n",
    "    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        for j in range(n + 1):\n",
    "            if i == 0:\n",
    "                dp[i][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][j] = i\n",
    "            elif word1[i - 1] == word2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
    "    return dp[m][n]\n",
    "\n",
    "queries = [\"willl ypu bee there\", \"hullo warld\", \"time\", \"loveu\", \"neper knou\", \"make no sence\"]\n",
    "for query in queries:\n",
    "    corrected_query = \"\"\n",
    "    for term in query.split():\n",
    "        corrected_query += str(correct_spellings(term, doc_dict)) + \" \"\n",
    "    print(\"Corrected Query is:\", corrected_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Michael-jackson-will-you-be-there-lyrics']\n"
     ]
    }
   ],
   "source": [
    "# Positional Index to support adjacent phrase queries\n",
    "class PositionalIndex:\n",
    "    def __init__(self, documents, doc_ids):\n",
    "        self.documents = documents\n",
    "        self.doc_ids = doc_ids\n",
    "        self.index = {}\n",
    "        for i, doc in enumerate(documents):\n",
    "            words = doc\n",
    "            for j, word in enumerate(words):\n",
    "                if word not in self.index:\n",
    "                    self.index[word] = {}\n",
    "                if doc_ids[i] not in self.index[word]:\n",
    "                    self.index[word][doc_ids[i]] = []\n",
    "                self.index[word][doc_ids[i]].append(j)\n",
    "\n",
    "    def search(self, query):\n",
    "        query_terms = query.split()\n",
    "        matching_doc_ids = []\n",
    "        if query_terms[0] in self.index:\n",
    "            for doc_id, positions in self.index[query_terms[0]].items():\n",
    "                for pos in positions:\n",
    "                    found = True\n",
    "                    for i in range(1, len(query_terms)):\n",
    "                        if doc_id not in self.index.get(query_terms[i], {}) or pos + i not in self.index[query_terms[i]][doc_id]:\n",
    "                            found = False\n",
    "                            break\n",
    "                    if found:\n",
    "                        matching_doc_ids.append(doc_id)\n",
    "                        break\n",
    "        return matching_doc_ids\n",
    "\n",
    "pos_index = PositionalIndex(df['Tokens'].tolist(), df['DocumentID'].tolist())\n",
    "\n",
    "query = 'will you be there'\n",
    "matching_docs = pos_index.search(query)\n",
    "print(matching_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reason to consider spell check and positional index (adjacent words):\n",
    "Most of the users will remember lyrics as adjacent phrases. So its better to check if the words appear in order for a phrase query by user.\n",
    "While typing the user might make typos. So check for those as well.\n",
    "Look at example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Query is: will you be \n",
      "Matching Documnets are: ['Michael-jackson-will-you-be-there-lyrics', 'Lana-del-rey-caught-you-boy-lyrics']\n"
     ]
    }
   ],
   "source": [
    "query = \"will ypu bee\"\n",
    "corrected_query = \"\"\n",
    "for term in query.split():\n",
    "    corrected_query += str(correct_spellings(term, doc_dict)) + \" \"\n",
    "print(\"Corrected Query is:\", corrected_query)\n",
    "matching_docs = pos_index.search(corrected_query)\n",
    "print(\"Matching Documnets are:\", matching_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now our search becomes easy and limited to less number of documnets to rank\n"
     ]
    }
   ],
   "source": [
    "print(\"Now our search becomes easy and limited to less number of documnets to rank\" \"\\U0001f600\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FihN3qniN-Jy"
   },
   "source": [
    "# Collaboration Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EKp_Fe-N-Jy"
   },
   "source": [
    "** You should fill out your collaboration declarations here.**\n",
    "\n",
    "**Reminder:** You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by filling out the Collaboration Declarations at the bottom of this notebook.\n",
    "\n",
    "Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found helpful code on stack overflow for reading json file  \n",
    "I found list of punctuation that can be removed on google  \n",
    "I found helpful code on google to remove stopwords  \n",
    "I found helpful first draft of structured code for inverted index to support Boolean Retrieval on chatGPT  \n",
    "I found helpful first draft of theoretical questions like why zipf's law, comparing 3 algorithms on chatGPT  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
